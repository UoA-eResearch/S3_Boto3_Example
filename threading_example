#!/usr/bin/python3
import threading
import boto3
import os
import sys
from boto3.s3.transfer import TransferConfig
session = boto3.session.Session()

def load_keys(filename):
  '''
  Load the keys and bucket details from the json configuration file
  This is kept separate, so the keys can be exclude from the github repository
  .gitignore file line
  conf/keys.json
  :param filename: the configuration filename
  '''
  # Our configuration files are in the current directory's conf/ directory
  if filename.startswith('/'):
    # filename is absolute
    conf_file = filename
  else:
    # Filename is relative to the script directory
    conf_file = os.path.abspath( os.path.join( os.path.dirname(__file__), filename ) )
  try:
    with open( conf_file ) as f:
      conf = json.load(f)
  except Exception as e:
    print( f"load_keys({filename}): ", e )
    raise SystemExit(1)
  return conf

conf = load_keys('conf/keys.json')
s3 = session.resource(
    's3',
    aws_access_key_id=conf['access_key'],
    aws_secret_access_key=conf['secret_key'],
    aws_session_token=None,
    region_name='us-east-1',
    use_ssl=False,
    endpoint_url=conf['endpoint'],
    config=None
)
BUCKET_NAME = conf['bucket']

CHUNK_SIZE = 5242880 #5M (minimum)

def multi_part_upload_with_s3(filename):
  # Multipart upload
  config = TransferConfig(multipart_threshold=CHUNK_SIZE, max_concurrency=10,
                          multipart_chunksize=CHUNK_SIZE, use_threads=True)
  file_path = os.path.join(os.path.dirname(__file__), f'data/{filename}')
  key_path = f'multipart_test2/{filename}'
  s3.meta.client.upload_file(file_path, BUCKET_NAME, key_path,
                          ExtraArgs={'ACL': 'public-read', 'ContentType': 'text/pdf'},
                          Config=config,
                          Callback=ProgressPercentage(file_path)
                          )

class ProgressPercentage(object):
  def __init__(self, filename):
      self._filename = filename
      self._size = float(os.path.getsize(filename))
      self._seen_so_far = 0
      self._lock = threading.Lock()

  def __call__(self, bytes_amount):
        # To simplify we'll assume this is hooked up
        # to a single filename.
        with self._lock:
            self._seen_so_far += bytes_amount
            percentage = (self._seen_so_far / self._size) * 100
            sys.stdout.write(
                "\r%s  %s / %s  (%.2f%%)" % (
                    self._filename, self._seen_so_far, self._size,
                    percentage))
            sys.stdout.flush()

if __name__ == '__main__':
  multi_part_upload_with_s3('test.tif')
